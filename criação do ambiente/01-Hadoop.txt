Install Hadoop 2.6 -- Virtual Box Ubuntu 16.04 LTS  -- Single Node Pseudo Mode
	1. Install Java
	2. Disable IPv6
	3. Add a dedicated Hadoop User
	4. Install SSH
	5. Give hduser Sudo Permission
	6. Set up SSH Certifiactes
	7. Install Hadoop
	8. Configure Hadoop
		a. bash.rc
		b. hadoop-env.sh
		c. core-site.xml
		d. mapred-site.xml.template
		e. hdfs-site.xml
	9. Format Hadoop filesystem
	10. Start Hadoop 
	11. Testing that is running
	12. Stopping Hadoop

#Install Java

-- atualiza pacotes ubuntu
sudo apt-get update

-- instala java
sudo apt-get install default-jdk

-- verificar versao java 
java -version

--OpenJDK Runtime Environment (build 1.8.0_121-8u121-b13-0ubuntu1.16.04.2-b13)

#Disable IPv6

-- instala o VIM
sudo apt-get install vim

-- abrir o arquivo /etc/sysctl.conf para edicao
sudo vim /etc/sysctl.conf

-- colar as linhas abaixo ao final do arquivo
# disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1

-- O comando abaixo deve retornar com o valor "1" apos reiniciar a maquina. Se rodar o comando antes de reiniciar, o valor retornara "0".
cat /proc/sys/net/ipv6/conf/all/disable_ipv6 

#Adding a dedicated Hadoop User

-- criar o grupo hadoop
sudo addgroup hadoop

-- criar usuario hduser com a senha hduser e adicionar ao grupo hadoop 
sudo adduser --ingroup hadoop hduser

#Install SSH

sudo apt-get install ssh

#Give hduser Sudo Permission

sudo adduser hduser sudo

#Setup SSH Certificates

su hduser

#inicio do bloco de comandos incluidos

#Create the .ssh directory:
	mkdir ~/.ssh

#Set the right permissions:
	chmod 700 ~/.ssh

#Create the authorized_keys file:
	touch ~/.ssh/authorized_keys

#Set the right permissions:
	chmod 600 ~/.ssh/authorized_keys
	
#fim do bloco de comandos incluidos

#rodar o comando dentro da pasta .ssh
ssh-keygen -t rsa -P ""

cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

ssh localhost

#Instaar o Hadoop

su hduser

-- faz o download do hadoop
wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz

-- descompacta o hadoop
tar xvzf hadoop-2.7.4.tar.gz

-- muda o diretorio para o hadoop
cd hadoop-2.7.4

-- cria o home do hadoop
sudo mkdir /usr/local/hadoop

-- move os arquivos pro home do hadoop
sudo mv * /usr/local/hadoop

#Set up the Configuration files

#Inserir variveis de ambiente Hadoop no Bashrc

sudo vim ~/.bashrc

#HADOOP VARIABLES START
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:/usr/lib/jvm/java-8-openjdk-amd64/bin
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
#export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
export HADOOP_OPTS="-Djava.library.path=$HADOOP_COMMON_LIB_NATIVE_DIR"
export PATH=$PATH:/usr/local/hadoop/bin
#HADOOP VARIABLES END

source ~/.bashrc

#Atualizar arquivo hadoop-env.sh
  
sudo vim /usr/local/hadoop/etc/hadoop/hadoop-env.sh

-- inserir a linha ao final do arquivo. Se tiver um JAVA_HOME no arquivo, comentar.
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

#ALterar arquivo core-site.xml

sudo mkdir -p /app/hadoop/tmp

sudo chown hduser:hadoop /app/hadoop/tmp

sudo vim /usr/local/hadoop/etc/hadoop/core-site.xml

-- inserir o contedo abaixo entre as tags configuration
<property>
	<name>hadoop.tmp.dir</name>
	<value>/app/hadoop/tmp</value>
	<description>A base for other temporary directories.</description>
</property>

<property>
	<name>fs.default.name</name>
	<value>hdfs://localhost:54310</value>
	<description>The name of the default file system.  A URI whose
		scheme and authority determine the FileSystem implementation.  The
		uri's scheme determines the config property (fs.SCHEME.impl) naming
		the FileSystem implementation class.  The uri's authority is used to
		determine the host, port, etc. for a filesystem.</description>
</property>

#Editar o arquivo mapred-site.xml

cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml

sudo vim /usr/local/hadoop/etc/hadoop/mapred-site.xml

<property>
	<name>mapred.job.tracker</name>
	<value>localhost:54311</value>
	<description>The host and port that the MapReduce job tracker runs
		at.  If "local", then jobs are run in-process as a single map
		and reduce task.
	</description>
</property>

#Editar o arquivo hdfs-site.xml

sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode

sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode

sudo chown -R hduser:hadoop /usr/local/hadoop_store

sudo vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml

<property>
	<name>dfs.replication</name>
	<value>1</value>
	<description>Default block replication.
		The actual number of replications can be specified when the file is created.
		The default is used if replication is not specified in create time.
	</description>
</property>
 
<property>
	<name>dfs.namenode.name.dir</name>
	<value>file:/usr/local/hadoop_store/hdfs/namenode</value>
</property>
 
<property>
	<name>dfs.datanode.data.dir</name>
	<value>file:/usr/local/hadoop_store/hdfs/datanode</value>
</property>

#Format Hadoop filesystem

-- reiniciar a VM antes de rodar o comando abaixo para aplicar as alteracoes no sistema.

hadoop namenode -format

#Starting Hadoop

su hduser

sudo chown -R hduser:hadoop /usr/local/hadoop/
	
cd /usr/local/hadoop/sbin

start-all.sh

#Testing if it is working 
	
jps

netstat -plten | grep java

http://localhost:50070/

#Stopping Hadoop

stop-all.sh

sudo cp /usr/local/hadoop/sbin/start-all.sh  /etc/init.d/

sudo chmod +x /etc/init.d/start-all.sh

sudo update-rc.d start-all.sh defaults